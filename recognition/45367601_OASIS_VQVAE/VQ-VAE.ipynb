{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorda\\miniconda3\\envs\\prac2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "tf.keras.mixed_precision.set_global_policy(\"float32\")\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r\"C:\\Users\\jorda\\Documents\"\n",
    "def load_data(path):\n",
    "    train_files = []\n",
    "    train_loc = os.path.join(path, \"\\keras_png_slices_data\\keras_png_slices_train\")\n",
    "    for filename in os.listdir(train_loc):\n",
    "        train_files.append(os.path.join(train_loc, filename))\n",
    "\n",
    "    test_files = []\n",
    "    test_loc = os.path.join(path, \"\\keras_png_slices_data\\keras_png_slices_test\")\n",
    "    for filename in os.listdir(test_loc):\n",
    "        test_files.append(os.path.join(test_loc, filename))\n",
    "\n",
    "    val_files = []\n",
    "    val_loc = os.path.join(path, \"\\keras_png_slices_data\\keras_png_slices_validate\")\n",
    "    for filename in os.listdir(val_loc):\n",
    "        val_files.append(os.path.join(val_loc, filename))        \n",
    "    \n",
    "    return train_files, test_files, val_files\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 544 files belonging to 1 classes.\n",
      "Found 9664 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "root_dir = r\"C:\\Users\\jorda\\Documents\"\n",
    "directories = [\"keras_png_slices_data\\keras_png_slices_test\",\n",
    "            \"keras_png_slices_data\\keras_png_slices_train\",\n",
    "            \"keras_png_slices_data\\keras_png_slices_validate\"]\n",
    "def load_data2(path, directories, batches):\n",
    "    test_imgs = keras.utils.image_dataset_from_directory(os.path.join(path, directories[0]),\n",
    "                                                        color_mode='grayscale',\n",
    "                                                        label_mode = None,\n",
    "                                                        batch_size=batches,\n",
    "                                                        image_size=(256, 256))       \n",
    "\n",
    "    train_imgs = keras.utils.image_dataset_from_directory(os.path.join(path, directories[1]),\n",
    "                                                        color_mode='grayscale',\n",
    "                                                        label_mode = None,\n",
    "                                                        batch_size=batches,\n",
    "                                                        image_size=(256, 256))       \n",
    "\n",
    "    # val_imgs = keras.utils.image_dataset_from_directory(os.path.join(path, directories[2]),\n",
    "    #                                                     color_mode='grayscale',\n",
    "    #                                                     label_mode = None,\n",
    "    #                                                     batch_size=batches,\n",
    "    #                                                     image_size=(256, 256))       \n",
    "    # print(type(test_imgs))\n",
    "    \n",
    "    # test_imgs = test_imgs.map(scale_down, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # train_imgs = train_imgs.map(scale_down, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # val_imgs = val_imgs.map(scale_down, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # var = tfp.stats.variance(test_imgs)\n",
    "    # print(\"variance is \", var)\n",
    "    return test_imgs, train_imgs\n",
    "\n",
    "# def scale_down(image):\n",
    "#     return image/255\n",
    "\n",
    "test_imgs, train_imgs = load_data2(root_dir, directories, None)\n",
    "\n",
    "train_np = np.stack(list(train_imgs))\n",
    "test_np = np.stack(list(test_imgs))\n",
    "# train_np = np.expand_dims(train_np, -1)\n",
    "# test_np = np.expand_dims(test_np, -1)\n",
    "# normalise\n",
    "train_np = train_np/255\n",
    "test_np = test_np/255\n",
    "\n",
    "var = np.var(train_np)\n",
    "# print(var)\n",
    "# # test_var = tf.\n",
    "# print(type(test_imgs))\n",
    "\n",
    "# print(type(train_np), train_np.shape)\n",
    "# print(type(test_np), test_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the VQ layer to quantise encoder output\n",
    "# Based on official keras documentation: https://keras.io/examples/generative/vq_vae/\n",
    "\n",
    "class VectorQuantizer(layers.Layer):\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # The `beta` parameter is best kept between [0.25, 2] as per the paper.\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "                                        initial_value=w_init(\n",
    "                                        shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"),\n",
    "                                        trainable=True,\n",
    "                                        name=\"embeddings_vqvae\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate the input shape of the inputs and\n",
    "        # then flatten the inputs keeping `embedding_dim` intact.\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
    "\n",
    "        # Quantization.\n",
    "        encoding_indices = self.get_code_indices(flattened)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
    "\n",
    "        # Reshape the quantized values back to the original input shape\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Calculate vector quantization loss and add that to the layer. You can learn more\n",
    "        # about adding losses to different layers here:\n",
    "        # https://keras.io/guides/making_new_layers_and_models_via_subclassing/. Check\n",
    "        # the original paper to get a handle on the formulation of the loss function.\n",
    "        commitment_loss = tf.reduce_mean((tf.stop_gradient(quantized) - x) ** 2)\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        self.add_loss(self.beta * commitment_loss + codebook_loss)\n",
    "\n",
    "        # Straight-through estimator.\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        # Calculate L2-normalized distance between the inputs and the codes.\n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Derive the indices for minimum distances.\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        return encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(latent_dim=16):\n",
    "    encoder_inputs = keras.Input(shape=(256, 256, 1))\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "    # x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    # x = layers.Conv2D(128, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    encoder_outputs = layers.Conv2D(latent_dim, 1, padding=\"same\")(x)\n",
    "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder(latent_dim=16):\n",
    "    latent_inputs = keras.Input(shape=get_encoder(latent_dim).output.shape[1:])\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(latent_inputs)\n",
    "    # x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    # x = layers.Conv2DTranspose(16, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, padding=\"same\")(x)\n",
    "    return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 128, 128, 32)      320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 64, 64, 16)        1040      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,856\n",
      "Trainable params: 19,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 64, 64, 16)]      0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 128, 128, 64)     9280      \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 256, 256, 32)     18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 256, 256, 1)      289       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,033\n",
      "Trainable params: 28,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"vq_vae\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 256, 256, 1)]     0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 64, 64, 16)        19856     \n",
      "                                                                 \n",
      " vector_quantizer (VectorQua  (None, 64, 64, 16)       1024      \n",
      " ntizer)                                                         \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 256, 256, 1)       28033     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,913\n",
      "Trainable params: 48,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_vqvae(latent_dim=16, num_embeddings=64):\n",
    "    vq_layer = VectorQuantizer(num_embeddings, latent_dim, name=\"vector_quantizer\")\n",
    "    # vq_layer.summary()\n",
    "    encoder = get_encoder(latent_dim)\n",
    "    encoder.summary()\n",
    "    decoder = get_decoder(latent_dim)\n",
    "    decoder.summary()\n",
    "    inputs = keras.Input(shape=(256, 256, 1))\n",
    "    encoder_outputs = encoder(inputs)\n",
    "    quantized_latents = vq_layer(encoder_outputs)\n",
    "    reconstructions = decoder(quantized_latents)\n",
    "    \n",
    "    return keras.Model(inputs, reconstructions, name=\"vq_vae\")\n",
    "\n",
    "\n",
    "get_vqvae().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAETrainer(keras.models.Model):\n",
    "    def __init__(self, train_variance, latent_dim=32, num_embeddings=128, **kwargs):\n",
    "        super(VQVAETrainer, self).__init__(**kwargs)\n",
    "        self.train_variance = train_variance\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.vqvae = get_vqvae(self.latent_dim, self.num_embeddings)\n",
    "\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.vq_loss_tracker = keras.metrics.Mean(name=\"vq_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.vq_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Outputs from the VQ-VAE.\n",
    "            reconstructions = self.vqvae(x)\n",
    "\n",
    "            # Calculate the losses.\n",
    "            reconstruction_loss = (tf.reduce_mean((x - reconstructions) ** 2) / self.train_variance)\n",
    "            total_loss = reconstruction_loss + sum(self.vqvae.losses)\n",
    "\n",
    "        # Backpropagation.\n",
    "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
    "\n",
    "        # Loss tracking.\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))\n",
    "\n",
    "        # Log results.\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"vqvae_loss\": self.vq_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 256, 256, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 128, 128, 32)      320       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 64, 64, 16)        1040      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,856\n",
      "Trainable params: 19,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 64, 64, 16)]      0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 128, 128, 64)     9280      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 256, 256, 32)     18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 256, 256, 1)      289       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,033\n",
      "Trainable params: 28,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "604/604 [==============================] - 29s 43ms/step - loss: 26.6034 - reconstruction_loss: 0.2696 - vqvae_loss: 26.1673\n",
      "Epoch 2/30\n",
      "604/604 [==============================] - 26s 43ms/step - loss: 1.2608 - reconstruction_loss: 0.1400 - vqvae_loss: 1.1290\n",
      "Epoch 3/30\n",
      "604/604 [==============================] - 26s 43ms/step - loss: 0.8351 - reconstruction_loss: 0.1061 - vqvae_loss: 0.6951\n",
      "Epoch 4/30\n",
      "604/604 [==============================] - 26s 43ms/step - loss: 0.1509 - reconstruction_loss: 0.0410 - vqvae_loss: 0.1060\n",
      "Epoch 5/30\n",
      "604/604 [==============================] - 26s 44ms/step - loss: 0.0953 - reconstruction_loss: 0.0343 - vqvae_loss: 0.0606\n",
      "Epoch 6/30\n",
      "604/604 [==============================] - 26s 44ms/step - loss: 0.0935 - reconstruction_loss: 0.0336 - vqvae_loss: 0.0596\n",
      "Epoch 7/30\n",
      "604/604 [==============================] - 26s 44ms/step - loss: 0.0939 - reconstruction_loss: 0.0320 - vqvae_loss: 0.0613\n",
      "Epoch 8/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0823 - reconstruction_loss: 0.0293 - vqvae_loss: 0.0527\n",
      "Epoch 9/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0803 - reconstruction_loss: 0.0278 - vqvae_loss: 0.0520\n",
      "Epoch 10/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0709 - reconstruction_loss: 0.0242 - vqvae_loss: 0.0459\n",
      "Epoch 11/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0625 - reconstruction_loss: 0.0226 - vqvae_loss: 0.0397\n",
      "Epoch 12/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0615 - reconstruction_loss: 0.0217 - vqvae_loss: 0.0395\n",
      "Epoch 13/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0624 - reconstruction_loss: 0.0210 - vqvae_loss: 0.0411\n",
      "Epoch 14/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0617 - reconstruction_loss: 0.0203 - vqvae_loss: 0.0413\n",
      "Epoch 15/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0607 - reconstruction_loss: 0.0196 - vqvae_loss: 0.0410\n",
      "Epoch 16/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0612 - reconstruction_loss: 0.0192 - vqvae_loss: 0.0420\n",
      "Epoch 17/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0599 - reconstruction_loss: 0.0186 - vqvae_loss: 0.0411\n",
      "Epoch 18/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0551 - reconstruction_loss: 0.0174 - vqvae_loss: 0.0374\n",
      "Epoch 19/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0500 - reconstruction_loss: 0.0163 - vqvae_loss: 0.0332\n",
      "Epoch 20/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0451 - reconstruction_loss: 0.0152 - vqvae_loss: 0.0296\n",
      "Epoch 21/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0414 - reconstruction_loss: 0.0141 - vqvae_loss: 0.0271\n",
      "Epoch 22/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0397 - reconstruction_loss: 0.0129 - vqvae_loss: 0.0266\n",
      "Epoch 23/30\n",
      "604/604 [==============================] - 27s 44ms/step - loss: 0.0364 - reconstruction_loss: 0.0118 - vqvae_loss: 0.0243\n",
      "Epoch 24/30\n",
      "604/604 [==============================] - 26s 42ms/step - loss: 0.0339 - reconstruction_loss: 0.0109 - vqvae_loss: 0.0229\n",
      "Epoch 25/30\n",
      "604/604 [==============================] - 26s 42ms/step - loss: 0.0334 - reconstruction_loss: 0.0104 - vqvae_loss: 0.0229\n",
      "Epoch 26/30\n",
      "604/604 [==============================] - 26s 42ms/step - loss: 0.0338 - reconstruction_loss: 0.0102 - vqvae_loss: 0.0236\n",
      "Epoch 27/30\n",
      "604/604 [==============================] - 26s 42ms/step - loss: 0.0344 - reconstruction_loss: 0.0100 - vqvae_loss: 0.0244\n",
      "Epoch 28/30\n",
      "604/604 [==============================] - 26s 42ms/step - loss: 0.0348 - reconstruction_loss: 0.0099 - vqvae_loss: 0.0249\n",
      "Epoch 29/30\n",
      "604/604 [==============================] - 26s 42ms/step - loss: 0.0355 - reconstruction_loss: 0.0098 - vqvae_loss: 0.0256\n",
      "Epoch 30/30\n",
      "604/604 [==============================] - 26s 42ms/step - loss: 0.0357 - reconstruction_loss: 0.0097 - vqvae_loss: 0.0259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x154541a3250>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae_trainer = VQVAETrainer(var, latent_dim=16, num_embeddings=128)\n",
    "vqvae_trainer.compile(optimizer=keras.optimizers.Adam())\n",
    "vqvae_trainer.fit(train_np, epochs=30, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('prac2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e1d54af1b355d71a80afa0c6078922cd3c310398571f5f0943216a19c79b4ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
