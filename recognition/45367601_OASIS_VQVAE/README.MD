# Generative model of the preprocessed OASIS brain dataset 
The vector quantified variational auto encoder (VQ-VAE) is a variation on the traditional variational autoencoder (VAE). This model was first described in the paper "Neural Discrete Representation Learning", by van der Oord et al., in 2017. 
## Traditional VAEs
A traditional variational autoencoder network consists of three key components: the encoder, a latent space, and a decoder network. The encoder effectively compresses the data to obtain the latent space representation, which is then decoded by the decoder network. An image of a traditional VAE network is shown in ][][][][][image]

https://miro.medium.com/max/1400/1*Qd1xKV9o-AnWtfIDhhNdFg@2x.png


## VQ-VAEs
The VQ-VAE differs from a traditional VAE by using a discretised version of the latent space; continuous encoder outputs are mapped (in the latent embedding space) to a series of K embedding vectors (each with dimensionality D). This mapping is done by means of a nearest-neighbour lookup (i.e., using the L2-norm). The vq encoding of the image is the input to the decoder network. 
These embedding vectors themselves are part of the learning process; the vectors are adjusted according to the l2 error terms (i.e., they are moved to more closely fit the data). 
The encoder/decoder networks learn in tandem; the encoder learns to produce optimal encodings for images, and the decoder learns to decode these, to produce accurate reconstructions from the encodings. The structure of a VQ-VAE is displayed in [][][][].

As can be seen the encoder is a convolutional neural network - to achieve compression of the images via this encoder network, the number of '


The general intention of using the VQVAE (and, more generally, VAEs) is often for generative tasks. The purpose of this implementation was to produce novel images which replicate the appearance of the supplied dataset (in this case, the OASIS dataset). This is achieved by:
1) Training the VQ-VAE (i.e., the encoder, decoder and vector-quantiser) until reconstructed images are suitably accurate (this includes adjusting hyperparameters related to the latent space, and potentially layers within the encoder/decoder)
2) Training a model to produce latent-space encodings using sampled latent vectors
3) Sampling latent-space vectors, passing these into the model trained in (2) to produce latent-space encodings with similar structure to the actual encodings
4) Pass the created encodings to the VQ-VAE decoder to produce an image from the encoding.

For the purpose of this implementation, a PixelCNN (as per the original paper) has been utilised for (2). This model is described below.
## Pixel CNN
The PixelCNN model was described in "Conditional Image Generation with PixelCNN Decoders" in 2016 by van der Oord et al. The primary goal of the PixelCNN in this case is (as described above) to learn the distribution of encoded samples (based on the encodings of the training dataset), and use this to generate new/novel encoded representations, which can then be decoded by the decoder of the VQ-VAE. The PixelCNN is regarded as an auto-regressive model; outputs are generated by sliding a convolutional kernal over the input images (from left to right). This is achieved on a pixel-by-pixel basis; the PixelCNN generates each new pixel of the image based on the pixels which come before it (in a left to right ordering, see image). 
https://towardsdatascience.com/autoregressive-models-pixelcnn-e30734ede0c1

Critically, a standard convolutional kernal in the Conv2D layer considers all of the pixels surrounding the pixel to be generated, and therefore considers pixels which come after the new pixel. This issue is overcome through the use of masking. The first type of mask (type A) is used in the first convolutional layer of the network and considers only prior pixels. The second type (type B) is used in subsequent convolutional layers and considers both prior pixels and the newly generated pixel (i.e., accounts for the pixel prediction formed by the first layer). The below graphic descibes this .
https://towardsdatascience.com/autoregressive-models-pixelcnn-e30734ede0c1


## Dataset - the preprocessed OASIS MRI dataset
The datset utilised was the preprocessed OASIS brain scan dataset supplied on the blackboard site (at this link: https://cloudstor.aarnet.edu.au/plus/s/tByzSZzvvVh0hZA/download). The dataset has been preprocessed, such that all scans are centered and 256*256 pixels. The dataset contains 9664 training images, 544 testing images and 1120 validation images. The images were normalised, by dividing all values by 255.

## The Model
The produced model was based on the open-source solution available in the Keras documentation (at https://keras.io/examples/generative/vq_vae/). Modifications to this code were made, to allow processing of the data on lower-spec hardware (it is able to run on a 6GB RTX3060 Laptop GPU) - this involved adaptation of the code to utilise batching and attempts at implementing mixed-precision. Additional modifications to the structure of the CNN and VQ-VAE were also trialled, including addition of Dropout and BatchNormalisation layers, in an attempt to battle poor generalisability.

### Encoder


### Decoder


### Vector Quantiser


### PixelCNN
Pixel - uses the conv and the masking to prevent the model seeing the exact pixel being replicated - it sees only what has come before it

## Performance
The VQ-VAE component of the generative task performed well; it was capable of reproducing the brain scans with a SSIM averaging well above 0.8 (typically, closer to 0.95 on the tested images). A series of images, alongside their respective reconstructions, are shown below. 

The PixelCNN generative model did not perform as hoped; the randomly generated encodings did not replicate the structure of the encodings from the brain images, and therefore the decoded images did not closely resemble real brains. Some of the generated brain models are shown below.


Notably, it appears that some aspects were retained (the 'wrinkles' in grey matter, the defined outer ring, etc.), but the shape of the brains was not learnt effectively, even when the Pixel was trained for 600+ epochs. One potential reason for this is that the model could not be trained on the full 9000+ brains in the training set; only approx. 2000 were able to be run at a time. 